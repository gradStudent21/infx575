{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As before, make sure gensim is installed\n",
    "# Usually pip install gensim, but YMMV\n",
    "\n",
    "from gensim.models import word2vec,doc2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Each document is in a separate text file\n",
    "# Use the glob module to work with file names\n",
    "import glob \n",
    "# smart_open helps with streaming data from files\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['describe', 'the', 'mid', 'term', 'transient', 'activity', 'markovian', 'systems', 'that', 'tend', 'to', 'silence', 'with', 'high', 'probability', 'in', 'the', 'long', 'run'], tags=['nipstxt/nips00/0192.txt']), TaggedDocument(words=['iring', 'rates', 'as', 'such', 'rates', 'would', 'be', 'zero', 'in', 'most', 'our', 'cases', 'in', 'the', 'ollowing', 'models', 'time', 'will', 'proceed', 'in', 'discrete', 'steps'], tags=['nipstxt/nips00/0192.txt'])]\n"
     ]
    }
   ],
   "source": [
    "# Write a function to handle the pre-processing \n",
    "# We will write this as a generator so it streams the data to conserve memory\n",
    "\n",
    "def read_corpus(globexpr, blocksize=2):\n",
    "    # Read and process all files that match the provided glob expression\n",
    "    # assumes all text files are encoded as iso-8859-1\n",
    "    for fname in glob.iglob(globexpr, recursive=True):\n",
    "        with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "            block = []\n",
    "            for i, line in enumerate(f):\n",
    "                if i % blocksize != blocksize - 1: \n",
    "                    cleanline = utils.simple_preprocess(line)\n",
    "                    if not cleanline: continue\n",
    "                    block = block + cleanline\n",
    "                else:\n",
    "                    #yield doc2vec.TaggedDocument(block, [\"%s:%s\" % (fname,i//blocksize)])\n",
    "                    yield doc2vec.TaggedDocument(block, [fname])\n",
    "                    block = []\n",
    "\n",
    "# Make a list of all the files from NIPS 2005\n",
    "docs = [doc for doc in read_corpus(\"nipstxt/nips*/*.txt\",blocksize=3)]\n",
    "\n",
    "# Check to see that the output looks sensible\n",
    "print(docs[20:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/m,d100,n5,w6,mc5,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "# Train a Doc2Vec model using the extracted documents\n",
    "# (You can skip this cell if you've already saved the model previously)\n",
    "model = doc2vec.Doc2Vec(docs, alpha=.025, min_alpha=.025, workers=4, window=6, min_count=5, iter=7, dm=1, dbow_words=1)\n",
    "\n",
    "print(model)\n",
    "model.save('nips.model.small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/m,d100,n5,w6,mc5,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "# Load the model from a file\n",
    "model = doc2vec.Doc2Vec.load('nips.model.small')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('steps', 0.6051098108291626),\n",
       " ('iteration', 0.5607196092605591),\n",
       " ('epoch', 0.4857795834541321),\n",
       " ('subsection', 0.47750017046928406),\n",
       " ('sweep', 0.45312538743019104),\n",
       " ('recursion', 0.4438837170600891),\n",
       " ('recursions', 0.44324538111686707),\n",
       " ('minute', 0.4330310821533203),\n",
       " ('stage', 0.4324946105480194),\n",
       " ('keystroke', 0.4307841658592224)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the terms most similar to \"svm\"  \n",
    "model.most_similar(positive=['step'], negative=[], topn=10)\n",
    "\n",
    "# What do we expect to see?\n",
    "# Note: Here we are using the word2vec model, not the doc2vec model.  \n",
    "# Remember word2vec vectors are an input to doc2vec.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a dictionary so we can look up the original documents by tag \n",
    "# (I'm not sure why Doc2Vec doesn't provide this for you, but it doesn't seem to.)\n",
    "docdict = {}\n",
    "for i, d in enumerate(docs):\n",
    "    key = d.tags[0]\n",
    "    val = docdict.get(key, [])\n",
    "    val.append(d)\n",
    "    docdict[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar to ' ['2D', 'materials', 'used', 'in', 'quantum', 'computing'] '\n",
      "MATCH nipstxt/nips10/0280.txt 0.4887927174568176 \n",
      " boltzmann machine learning using mean field theory and linear response correction\n",
      "department of biophysics university of nijmegen geert grooteplein\n",
      "rodriguez instituto de ingenieria del conocimiento departamento de ingenieria informs tica\n",
      "abstract we present new approximate learning algorithm for boltzmann\n",
      "second order in the weights the linear response correction to the correlations is given by the hessian of the gibbs free energy the\n",
      "of neurons we compare the performance of the exact bm learning algorithm with first order weiss mean field theory and second\n",
      "connected ising spin glass model on neurons we conclude that the method works well for paramagnetic problems the tap\n",
      "theory both for paramagnetic and spin glass problems and that the inclusion of diagonal weights improves the weiss approximation\n",
      "introduction boltzmann machines bms are networks of binary neurons with stochastic\n",
      "between neurons the probability distribution over neuron states will become stationary and is given by the boltzmann gibbs distribution the boltzmann\n",
      "however computation of or any statistics involving such as mean firing rates or correlations requires exponential time in the number of neurons this is\n",
      "due to the fact that contains normalization term which involves sum over all states in the network of which there are exponentially many this problem\n",
      "using statistical sampling techiques learning can be significantly improved however the method has rather poor convergence and can only be applied to small\n",
      "in an acceleration method for learning in bms is proposed using mean field theory by replacing sisj by mimj in the learning rule it can be shown that\n",
      "general furthermore we argue that the correlations can be computed using the linear response theorem\n",
      "of convex functions jensen inequality and tangential bounds in this paper we present an alternative derivation which uses legendre transformation and small\n",
      "and higher can be computed in systematic manner and that it may be applicable to arbitrary graphical models\n",
      "the boltzmann machine is defined as follows the possible configurations of the network can be characterized by vector si sn where si is the\n",
      "using glauber dynamics let us define the energy of configuration as\n",
      "dent of time thermal equilibrium and is given by the boltzmann distribution exp\n",
      "distribution learning consists of adjusting the weights and thresholds in such way that\n",
      "possible suitable measure of the difference between the distributions and is the\n",
      "log\n",
      "the parameter is the learning rate the brackets and denote the free and clamped expectation values respectively\n",
      "the computation of both the free and the clamped expectation values is intractible because it consists of sum over all unclamped states as result the bm learning\n",
      "the mean field approximation we derive the mean field free energy using the small expansion as introduced by\n",
      "eint isi ein\n",
      "wijsisj ij\n",
      "log tr and is function of the independent variables wij oi and we perform legendre\n",
      "transformation on the variables oi by introducing mi ae\n",
      "given by interaction\n",
      "we expand\n",
      "eint ooi\n",
      "ant\n",
      "can directly compute\n",
      "\n",
      "ij\n",
      "ij thus\n",
      "log mi\n",
      "ij\n",
      "ij\n",
      "boltzmann machine learning using mean field theory where is some unknown function of\n",
      "oi og tanh_ mi wijmj lrti\n",
      "the correlations are given by\n",
      "we therefore obtain from eq with\n",
      "om ij ore\n",
      "ij ij rn mjw uj\n",
      "ing eqs and the correlations by their linear response approximations eqs the inclusion of hidden units is straigthforward one applies the above approximations\n",
      "due to the matrix inversion learning without hidden units\n",
      "let us define cij sisj si sj which can be directly computed from the data the fixed point equation for aoi gives\n",
      "the fixed point equation for awij using eq gives awij aij cij\n",
      "in our case we used fsolve from matlab subsequently we obtain oi from eq we refer to this method as the tap approximation\n",
      "thresholds in the same way as described above but without the terms of order in eqs and since this is the standard weiss mean field expression we refer to\n",
      "the fixed point equations are only imposed for the off diagonal elements of awij because the boltzmann distribution eq does not depend on the diagonal elements\n",
      "diagonal weight terms as is discussed there if we were to impose eq for as well we have if is invertible we therefore have however\n",
      "weights wii by adding the term wiimi to the righthandside of eq in the weiss approximation thus\n",
      "and oi is given by eq in the weiss approximation clearly this method is com putationally simpler because it gives an explicit expression for the solution of the\n",
      "kappen and rodrguez\n",
      "for the target distribution in eq we chose fully connected ising spin glass model with equilibrium distribution\n",
      "jijsisj ij\n",
      "this model is nown as the sherrington kirkpatrick sk model depending on the val\n",
      "ordered and spin glass frustrated phase for the para magnetic spin glass phase is obtained for we will assess the effectiveness of our\n",
      "realizable task the optimal kl divergence is zero which is indeed observed in our simulations\n",
      "fore this comparison is only feasible for small networks the reason is that the computation of the kullback divergence requires the computation of the boltzmann\n",
      "we present results for network of neurons for we generated\n",
      "matrix we computed the on all states for each of the problems we applied the tap method the weiss method and the weiss method with diagonal\n",
      "using conjugate gradient descent and verified that it gives kl divergence equal to zero as it should we also applied factorized model ii misi\n",
      "in fig la we show for each the average kl divergence over the problem instances as function of for the tap method the weiss method the weiss\n",
      "method gives the best results but that its performance deteriorates in the spin glass phase\n",
      "problem instance in fig lb we show the mean value of the kl divergence of the tap solution together with the minimum and maximum values obtained on the\n",
      "despite these large fluctuations the quality of the tap solution is consistently better than the weiss solution in fig lc we plot the difference between the tap\n",
      "in we concluded that the weiss solution with diagonal weights is better than the standard weiss solution when learning finite number of randomly generated\n",
      "without diagonal weights we observe again that the inclusion of diagonal weights leads to better results in the paramagnetic phase but leads to worse results\n",
      "either the matrix is not invertible or the kl divergence is infinite this problem becomes more and more severe for increasing we therefore have not presented\n",
      "boltzmann machine learning using mean field theory comparison mean values\n",
      "fact\n",
      "weiss tap\n",
      "\n",
      "difference weiss and tap\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tap mean\n",
      "max\n",
      "difference weiss and weiss\n",
      "problems for network of neurons comparison of mean kl divergences for the factorized model fact the weiss mean field approximation with and without\n",
      "of the exact method yields zero kl divergence for all the mean mini mum and maximum kl divergence of the tap approximation for the problem\n",
      "for the weiss approximation and the tap approximation as function of the mean difference between the kl divergence for the weiss approximation with\n",
      "discussion we have presented derivation of mean field theory and the linear response correc\n",
      "can in principle be computed to arbitrary order however one should expect that the solution of the resulting mean field and linear response equations will become\n",
      "be applicable to other network models such as the sigmoid belief network potts networks and higher order boltzmann machines\n",
      "this is intuitively clear since paramagnetic problems have unimodal probability distribution which can be approximated by mean and correlations around the\n",
      "useful approximation of the correlations when compared to the factorized model which ignores all correlations in this regime the tap approximation improves\n",
      "significantly on the weiss approximation one may therefore hope that higher order approximation may further improve the method for spin glass problems therefore\n",
      "unimodal distributions in order to further investigate this issue one should also study the ferromagnetic case which is multimodal as well but less\n",
      "it is interesting to note that the performance of the exact method is absolutely insensitive to the value of naively one might have thought that for highly\n",
      "from local minima apparently this is not the case the exact kl divergence has just one minimum but the mean field approximations of the gradients may have\n",
      "acknowledgement this research is supported by the technology foundation stw applied science\n",
      "references\n",
      "\n",
      "\n",
      "lo\n",
      "chines cognitive science itzykson and drouffe statistical field theory cambridge monographs on\n",
      "peterson and anderson mean field theory learning algorithm for neural networks complex systems\n",
      "space neural computation kappen and rodriguez efficient learning in boltzmann machines using\n",
      "parisi statistical field theory frontiers in physics addison wesley saul jaakkola and jordan mean field theory for sigmoid belief net\n",
      "plefka convergence condition of the tap equation for the infinite range ising spin glass model journal of physics \n",
      "\n",
      "MATCH nipstxt/nips07/0883.txt 0.44854623079299927 \n",
      " comparison of discrete time operator models for nonlinear system identification\n",
      "department of electrical and computer engineering university of queensland\n",
      "mail back act elec uq oz au abstract\n",
      "context of finite word length linear signal processing comparisons are made between the recently presented gamma operator model and the delta\n",
      "and prediction using neural networks new model based on an adaptive bilinear transformation which generalizes all of the above models is\n",
      "introduction the shift operator defined as qz frequently used to provide me dommn\n",
      "identification or time series prediction problems may be constructed common method of developing nonlinear system identification models is to use neural network architecture as\n",
      "of the network shift operators at the input of the network provide the regression vectors and in manner analogous\n",
      "it is known that linear models based on the shift operator suffer problems when used to model lightly damped low frequency ldlf systems with poles near on the unit\n",
      "round off noise become problem as the difference between successive sampled inputs becomes smaller and smaller\n",
      "method of overcoming this problem is to use an alternative discrete time operator agarwal and burros first proposed the use of the delta operator in digital filters to replace\n",
      "operator is defined as\n",
      "where is the discrete time sampling interval williamson showed that the delta operator allows better performance in terms of coefficient sensitivity for digital filters derived from\n",
      "filtering estimation and control more recently de vries principe at al proposed the gamma operator as means\n",
      "defined by\n",
      "it may be observed that it is generalization of the delta operator with adjustable parameters\n",
      "order operator was given in this raises the question is the gamma operator capable of providing better neural network\n",
      "may be better than these for nonlinear modelling and prediction using neural networks in the context of robust adaptive control palaniswami has introduced the rho operator\n",
      "the rho operator is defined as\n",
      "where are adjustable parameters the rho operator generalizes the delta and gamma\n",
      "operator when and the rho operator reduces to the delta operator for the rho operator is equivalent to the gamma operator\n",
      "allowing the derivation of simpler algorithms the operator can be considered as stable low pass filter and parameter estimation using the operator is low frequency\n",
      "unmodelled high frequency characteristics by defining the bilinear transformation blt as an operator it is possible to introduce\n",
      "operator as\n",
      "with the restriction that fi to ensure is not constant function the bilinear\n",
      "parameters each operator the pi operator can be reduced to each of the previous operators in the work reported here we consider these alternative discrete time operators in feed\n",
      "comparison of discrete time operator models for nonlinear system identification gamma model with other models based on the shift delta rho and pi operators\n",
      "are compared by simulation experiments\n",
      "processing model which generalizes the usual discrete time linear moving average model ie single\n",
      "\n",
      "shift operator\n",
      "gamma operator rho operator\n",
      "this general class of moving average model can be termed ma we define uo and ui ui and hence obtain\n",
      "aui ui ui cui ui\n",
      "ui ui ui\n",
      "delta operator gamma operator\n",
      "pi operator\n",
      "elements at the input stage the input vector to the network is\n",
      "perceptron or mlp model an mlp model having layers with no nodes per layer is defined in the\n",
      "\n",
      "where each neuron in layer has an output of layer consists of nl neurons\n",
      "bias is sigmoid function typically evaluated as tanh and synaptic connection between unit in the previous layer and unit in the current layer is represented by\n",
      "andrew back ah chung tsoi we consider employs the operator at the input layer only it would be feasible to use the\n",
      "on line algorithms to update the operator parameters in the ma model can be found readily in the case of the mlp model we approach the problem by backpropagating\n",
      "de vries and principe et al proposed stochastic gradient descent type algorithms for adjusting the operator coefficient using least squares error criterion for brevity\n",
      "be applied see for example we define an instantaneous output error criterion where\n",
      "we have gamma operator\n",
      "pi operator first order algorithm to update the coefficients is\n",
      "where the adjustment in weights is found as zx\n",
      "\n",
      "\n",
      "order sensitivity vector of the model operator parameters defined by gamma operator\n",
      "acj acj rhooperator piopermor\n",
      "substituting ui in om ursive uations for noting that\n",
      "aui la rho operator\n",
      "pi operator\n",
      "comparison of discrete time operator models for nonlinear system identification for the gamma rho and pi operators respectively and where bi refers to the jth element\n",
      "more powerful updating procedure can be obtained by using the gauss newton method in this case we replace with omitting subscripts for clarity\n",
      "where is the gain sequence see for details is weighting matrix which may\n",
      "\n",
      "bt\n",
      "matrix inversion lemma factorization methods such as cholesky decomposition or other fast algorithms using the well known matrix inversion lemma we substitute\n",
      "\n",
      "et al note that setting the coefficients for the gamma operator to unity provided the best approach for certain problems\n",
      "we are primarily interested in the differences between the operators themselves for mod elling and prediction and not the associated difficulties of training multilayer perceptrons\n",
      "comparison in this paper we test the models using single layer network hence these linear system examples are used to provide an indication of the operators performance\n",
      "the first problem considered is system identification task arising in the context of high bit rate echo cancellation in this case the system is described by\n",
      "this system has poles on the real axis at and thus it is an ldlf system\n",
      "variance gauss newton algorithm was used to determine all unknown weights we conducted monte carlo tests using runs of differently seeded training samples each of\n",
      "using the signal to noise ratio snr defined as log where is the expectation operator and is the desired signal for each run we used the last\n",
      "andrew back ah chung tsoi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "of\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "of\n",
      "\n",
      "\n",
      "\n",
      "on the following operators shift delta gamma rho and pi table system identification experiment results\n",
      "shift delta\n",
      "rho pi\n",
      "which provided stable convergence the values chosen for this experiment were for the gamma rho and pi operator models\n",
      "results for this experiment are shown in table and figure we observe that the pi operator gives the best performance overall some difficulties with instability occurring\n",
      "operator updates the next best performance was observed in the rho and then gamma models with fewer instability problems occurring\n",
      "the second experiment used model described by\n",
      "this system is rd order lowpass filter tested in the same experimental procedures as used in experiment were followed in this case\n",
      "comparison of discrete time operator models for nonlinear system identification table system identification experiment results\n",
      "shift delta\n",
      "rho pi\n",
      "problem is less it is observed that that the pi model is only slightly better than the gamma and rho models interestingly the gamma and rho models had no problems with stability\n",
      "the delta model gave wide variation in results and performed poorly from these and other experiments performed it appears that performance advantages can\n",
      "recorded runs the extra degrees of freedom in the rho and pi operators appear to provide the means to give better performance than the gamma model the improvements of the\n",
      "instabilities occurring in the operators and potentially multimodal mean square output error surface in the operator parameter space\n",
      "wider range of tasks we present these preliminary examples as an indication of how these alternative operators perform on some system identification problems\n",
      "models based on the delta operator rho operator and pi operator have been presented and new algorithms derived comparisons have been made to the previously presented\n",
      "applications while the simulation examples considered show are only linear it is important to realize\n",
      "these networks is identical to what we have considered here we treat only the linear case in the examples in order not to complicate our understanding of the results knowing that\n",
      "the results obtained indicate that the more complex operators provide potentially more powerful modelling structure though there is need for further work into mechanisms of\n",
      "the rho model was able to perform better than the gamma model on the problems tested and gave similar results in terms of susceptibility to convergence and instability roblems\n",
      "attention to ensure the stability of the coefficients for future work it would be of value to analyse the convergence of the algorithms in\n",
      "convergence of the model andrew back ah chung tsoi\n",
      "the first author acknowledges financial support from the australian research council the second author acknowledges partial support from the australian research council\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "agarwal and bums new recursive digital filter structures having very\n",
      "pp dec de vries principe theory for neural networks with time delays advances\n",
      "de vries principe and de oliveira adaline with adaptive recursive\n",
      "eds ieee press pp de vries principe the gamma model new neural model for temporal\n",
      "fan and li operator recursive gradient algorithm for adaptive signal processing proc ieee int conf acoust speech and signal proc vol hi pp\n",
      "ljung and derstrom theory and practice of recursive identification cam bridge massachusetts the mit press\n",
      "cliffs prentice hall peterka control of uncertain processes applied theory and algorithms ky\n",
      "palaniswami new discrete time operator for digital estimation and control the university of melbourne department of electrical engineering technical report\n",
      "palaniswami digital estimation and control with new discrete time operator proc th ieee conf decision and control pp\n",
      "cations with the focused gamma net advances in neural information processing systems vol pp\n",
      "of adaptive iir filters with restricted feedback ieee trans signal processing vol pp\n",
      "feedforward layered networks proc int joint conf neural networks seattle vol pp\n",
      "science and engineering englewood cliffs nj prentice hall shah and palmieri meka fast local algorithm for training feedfoward\n",
      "shynk adaptive iir filtering using parallel form realizations ieee trans acoust speech signal proc vol pp\n",
      "de silva de oliveira principe and de vries generalized feed forward filters with complex poles neural networks for signal processing ii\n",
      "williamson delay replacement in direct form structures ieee trans acoust speech signal processing vol assp pp aprl \n",
      "\n",
      "MATCH nipstxt/nips12/0435.txt 0.433463990688324 \n",
      " differentiating functions of the jacobian with respect to the weights\n",
      "nec research institute independence way\n",
      "flake research nj ne corn barak pearlmutter\n",
      "university of new mexico albuquerque nm\n",
      "abstract for many problems the correct behavior of model depends not only on\n",
      "matrix of partial derivatives of the model outputs with respect to its in puts we introduce the prop algorithm an efficient general method for\n",
      "the jacobian of model with respect to its free parameters the algorithm applies to any parametrized feedforward model including nonlinear re\n",
      "introduction let be an input output twice differentiable feedforward model parameterized\n",
      "of\n",
      "dx the algorithm we introduce can be used to optimize functions of the form\n",
      "\n",
      "where and are user defined constants our algorithm which we call prop can be used to calculate the exact value of both oeu ow or oev ow in times the\n",
      "to have specific first derivatives or for implementing several other well known algorithms such as double backpropagation and tangent prop\n",
      "formalism which we use to derive our algorithm is actually more interesting because it allows us to modify prop to easily be applicable to wide variety of model types and\n",
      "objective functions as such we spend fair portion of this paper describing the mathe matical framework from which we later build prop\n",
      "and motivation for why optimizing the properties of the jacobian is an important problem section introduces our formalism and contains the derivation of the prop algorithm\n",
      "further work and gives our conclusions background and motivation\n",
      "divided into works that are descriptive or prescriptive perhaps the best known descriptive result is due to white et al who show that given noise free data multilayer percep\n",
      "the number of training points goes to infinity the difficulty with applying this result is the strong requirements on the amount and integrity of the training data requirements which\n",
      "and kuo and deco and schfirmann who showed that using noisy training data from chaotic systems can lead to models that are accurate in the input output sense but inaccu\n",
      "the largest lyapunov exponent and the correlation dimension mlps are particularly problematic because large weights can lead to saturation at particu\n",
      "when evaluated near the center of the sigmoid transition several methods to combat this type of over fitting have been proposed one of the earliest methods weight decay\n",
      "for models in which the output is linear in the weights because minimizing the magnitude of the weights is equivalent to minimizing the magnitude of the model first derivatives\n",
      "cause large or small weights do not always correspond to having large or small first derivatives\n",
      "function equal to lcge cga ll training on this function results in form of regularization that is in many ways an elegant combination of weight decay and training with noise it is\n",
      "of the model unlike weight decay double backpropagation can be seen as special case of prop the algorithm derived in this paper\n",
      "simard et al introduced the tangent prop algorithm which was used to train mlps for optical character recognition to be insensitive to small affine transformations in the\n",
      "derivation we now define formalism under which prop can be easily derived the method is\n",
      "hessian of an mlp and an arbitrary vector however where pearlmutter used differential operators applied to model weight space we use differential operators defined with\n",
      "our entire derivation is presented in five steps first we will define an auxiliary error differentiating funca ons of the jacobian\n",
      "we will define special differential operator that can be applied to both the auxiliary error function and its gradient with respect to the weights we will then see that the result of\n",
      "to analytically calculating the derivatives required to optimize equations and we then show an example of the technique applied to an mlp finally in the last step the complete\n",
      "to avoid confusion when referring to generic data driven models the model will always be expressed as vector function where refers to the model input and to refers\n",
      "models while ignoring the mechanics of how the models work internally complementary to the generic vector notation the notation for an mlp uses only scalar symbols however\n",
      "inputs weights etc which can lead to some ambiguity to be clear when using vector notation the input and output of an mlp will always be denoted by and respectively\n",
      "when using scalar arithmetic the scalar notation for mlps will apply auxiliary error function\n",
      "to urf to\n",
      "property that utj which will be useful to the derivation shortly note that appears in the taylor expansion of about point in input space\n",
      "thus while holding the weights to fixed and letting ax be perturbation of the input equation characterizes how small changes in the input of the model change the value of\n",
      "be setting ax rv with being an arbitrary vector and being small value we can rearrange equation into the form\n",
      "lim rv\n",
      "this final expression will allow us to define the differential operator in the next subsection differential operator\n",
      "operator rv to\n",
      "flake and pearlmutter\n",
      "obeys all of the standard rules for differentiation the operator also yields the identity\n",
      "we will now see that the result of calculating to can be used to calculate both oe oto and oev oto note that equations all assume that both and are in\n",
      "to value that depends on both and to however the derivation still works because our choices are explicitly made in such way that the chain rule of differentiation is not\n",
      "despite the dependence to optimize with respect to equation we use\n",
      "\n",
      "in these equations superscripts denote the layer number starting at subscripts index\n",
      "is the net input coming into the same neuron the output of neuron at node layer and\n",
      "the feedback equations calculated with respect to are ui\n",
      "with jttt to optimize with respect to equation we use jv_b jv_b tlojv\n",
      "with jv method applied to mlps\n",
      "consider an mlp with layers of nodes defined by the equations fi\n",
      "differentiating functions of the jacobjan\n",
      "ijxj\n",
      "\n",
      "ow for\n",
      "\n",
      "operator to the feedforward equations yields\n",
      "\n",
      "rv wlij\n",
      "where the vi term is component in the vector from equation as the final step we apply the operator to the feedback equations which yields\n",
      "for\n",
      "owti yj yj\n",
      "\n",
      "implementing this algorithm is nearly as simple as implementing normal gradient descent for each type of variable that is used in an mlp net input neuron output weights thresh\n",
      "result of applying the operator to the original variable with this change in place the complete algorithm to compute oe ow is as follows\n",
      "set the mlp inputs to the value of that is to be evaluated at perform normal feedforward pass using equations and\n",
      "flake and pearlmutter co\n",
      "with excellent approximation of the derivative perform the feedback pass with equations note that values in the\n",
      "set to jtu perform rv forward pass with equations\n",
      "perform rv backward pass with equations after the last step the values in the ot owtij and terms contain the\n",
      "backward calculations are nearly identical to the typical output and gradient evaluations the forward and backward passes of the models used\n",
      "rv forward pass is performed between the normal forward and backward passes because can only be determined after the has been calculated\n",
      "to demonstrate the effectiveness and generality of the prop algorithm we have imple mented it on top of an existing neural network library in such way that the algorithm\n",
      "works and higher order networks we trained an mlp with ten hidden tanh nodes on points with conjugate gradient the\n",
      "cos our unknown function which the mlp never sees data from is sin sin the model quickly converges to solution in approximately iterations\n",
      "function the mlp yields poor approximation of the function but very accurate approx imation of the function derivative we could have trained on both outputs and derivatives\n",
      "differentiating functions of the jacobian conclusions\n",
      "the jacobian matrix of feedforward nonlinear systems the method can be easily applied to most nonlinear models in common use today the resulting algorithm prop can be easily\n",
      "uses include targeting known first derivatives implementing tangent prop and double backpropagation enforcing identical sensitivities in auto encoders deflating the largest\n",
      "source separation and building nonlinear controllers while some special cases of the prop algorithm have already been studied great deal\n",
      "lem some anecdotal evidence seems to imply that optimization of the jacobian can lead to better generalization and faster training it remains to be seen if prop used on nonlinear\n",
      "we thank frans coetzee yannis kevrekidis joe ruanaidh lucas parra scott rickard\n",
      "eric baum and the nec research institute for funding the time to write up these results references\n",
      "propagation ieee transactions on neural networks november simard victorri le cun and denker tangent prop formalism for\n",
      "hanson and richard lippmann editors advances in neural information process ing systems volume pages morgan kaufmann publishers inc\n",
      "with multilayer feedforward networks in halbert white editor artificial neural networks chapter pages blackwell cambridge mass\n",
      "mapping and its derivative in halbert white editor artificial neural networks chapter pages blackwell cambridge mass\n",
      "networks and the issues of dynamic modeling bifurcations and chaos deco and schfirmann dynamic modeling of chaotic time series in russell\n",
      "ing theory and natural learning systems volume iv of making learning systems practical chapter pages the mit press cambridge mass\n",
      "conf cognitive science society pages hillsdale nj erlbaum barak pearlmutter fast exact multiplication by the hessian neural computation\n",
      "flake industrial strength modeling tools submitted to nips flake and pearlmutter optimizing properties of the jacobian of nonlinear \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"2D materials used in quantum computing\".split()\n",
    "print(\"most similar to '\", query, \"'\")\n",
    "targetvec = model.infer_vector(query)\n",
    "#print(targetvec)\n",
    "for tag, sim in model.docvecs.most_similar([targetvec], topn=3):\n",
    "    #print(tag, sim)\n",
    "    #print(model.docvecs[tag])\n",
    "    snippet = \"\\n\".join([\" \".join(d.words) for d in docdict[tag]])\n",
    "    print(\"MATCH\", tag, sim, \"\\n\", snippet, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hamilton', 0.12619615),\n",
       " ('university', 0.038542304),\n",
       " ('oh', 0.038114462),\n",
       " ('van', 0.0131783),\n",
       " ('phys', 0.012615132),\n",
       " ('edu', 0.0083096744),\n",
       " ('planck', 0.0065921266),\n",
       " ('cowan', 0.0061705513),\n",
       " ('der', 0.0058846357),\n",
       " ('japan', 0.0053933854)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_output_word([\"hamilton\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import re\n",
    "yearpatt = 'nips(\\d\\d)'\n",
    "#paperpatt = '(\\d\\d\\d\\d)'\n",
    "years = [int(re.search(yearpatt, key).groups(0)[0]) for key in docdict.keys()]\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "X = model.docvecs.doctag_syn0\n",
    "Y = np.array(years)\n",
    "\n",
    "N = len(X)\n",
    "split = 9*N//10\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "training_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "\n",
    "trainX, testX = X[training_idx], X[test_idx]\n",
    "trainY, testY = Y[training_idx], Y[test_idx]\n",
    "\n",
    "regr.fit(trainX, trainY)\n",
    "\n",
    "predictY = regr.predict(testX)\n",
    "\n",
    "residuals = predictY - testY\n",
    "\n",
    "RSE = np.sqrt(sum((residuals**2/N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2822af51d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzfJ+NSuFZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f283f737da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.scatter(testY, predictY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
